{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b51441e7-495e-4f1e-9b81-2363d46ed3dd",
   "metadata": {},
   "source": [
    "# __Tests of Superior Predictive Ability__\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "__DATA 5610__ <br>\n",
    "Author:      Tyler J. Brough <br>\n",
    "Last Update: April 19, 2022 <br>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a101eeea-7f0f-4ea4-a668-9096c63ae315",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "__Notice__\n",
    "\n",
    "* These notes follow the notes by Kevin Sheppard very closely\n",
    "\n",
    "* See also:\n",
    "    - White, _Econometrica_ 2000\n",
    "    - Sullivan, Timmermann, and White, _Journal of Finance_ 1999\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0fd42a9-5516-41a9-8e21-80dc49b2c5c2",
   "metadata": {},
   "source": [
    "## __Overview__\n",
    "\n",
    "* Multiple hypothesis testing\n",
    "    - White's Reality Check (RC) and Hansen's Superior Predictive Test (SPA)\n",
    "    - StepM procedure (we won't cover)\n",
    "    - Model Confidence Set\n",
    "    - False Discovery Rate Control\n",
    " \n",
    "* Bayesian methods\n",
    "    - ROPE methods (Kruschke, Kuhn)\n",
    "    - Bandit models approaches (Reinforcement Learning approach)\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b247e2-2db9-4d17-87ad-c0fe819e7eef",
   "metadata": {},
   "source": [
    "## __White's Reality Check__\n",
    "\n",
    "* The _Reality Check_ extends DMW to testing for _Superior Predictive Ability_ (SPA)\n",
    "\n",
    "* Tests of SPA examine whether or not a set of predictive models can outperform a benchmark model\n",
    "\n",
    "* Suppose forecasts were available for $m$ forecasts $j = 1, \\ldots, m$\n",
    "\n",
    "* The vector of loss differentials _relative to a benchmark_ could be constructed\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "{\\large \\delta_{t} = \\begin{bmatrix}\n",
    "                      L(y_{t+h}, \\hat{y}_{t+h,BM|t}) - L(y_{t+h}, \\hat{y}_{t+h,1|t}) \\\\\n",
    "                      L(y_{t+h}, \\hat{y}_{t+h,BM|t}) - L(y_{t+h}, \\hat{y}_{t+h,2|t}) \\\\\n",
    "                      \\vdots \\\\\n",
    "                      L(y_{t+h}, \\hat{y}_{t+h,BM|t}) - L(y_{t+h}, \\hat{y}_{y+t,m|t}) \\\\\n",
    "                     \\end{bmatrix}}\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "* $\\hat{y}_{t+h, BM|t}$ is the loss from the _benchmark forecast_\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15aea78-c620-474c-a562-dfbc2e2f462b",
   "metadata": {},
   "source": [
    "## __Implementing the Reality Check__\n",
    "\n",
    "* The Reality Check is implemented using the $P$ by $m$ matrix of loss differentials\n",
    "    - $P$ out-of-sample periods\n",
    "    - $m$ models\n",
    "    \n",
    "* The original article describes two methods\n",
    "    - Monte Carlo Reality Check\n",
    "    - Bootstrap Reality Check\n",
    "    \n",
    "* In practice, only the Bootstrap Reality Check is used\n",
    "\n",
    "* The distribution of the _maximum_ of normals is not normal, and so only the percentile method is applicable\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f53175-15f5-4def-aad9-53b48ed1e386",
   "metadata": {},
   "source": [
    "### __The Algorithm (Bootstrap Reality Check)__\n",
    "\n",
    "---\n",
    "\n",
    "1. _Compute_ $\\quad T^{RC} = \\max{(\\bar{\\delta})}$ \n",
    "\n",
    "2. For $b = 1, \\ldots, B$ _re-sample the vector of loss differentials_ $\\mathbf{\\delta}_{t}$ _to construct a bootstrap sample_ $\\{\\mathbf{\\delta}_{b,t}^{\\ast} \\}$ _using the stationary bootstrap_\n",
    "\n",
    "3. _Using the bootstrap sample, compute_\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "{\\large T_{b}^{\\ast RC} = \\max{\\left( \\frac{1}{P} \\sum\\limits_{t=R+1}^{T} (\\mathbf{\\delta}_{b,t}^{\\ast} - \\mathbf{\\bar{\\delta}}) \\right) }}\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "4. _Compute the Reality Check p-value as the percentage of the bootstrapped maxima which are larger than the sample maximum_\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "{\\large p-\\mbox{value} = \\frac{1}{B} \\sum\\limits_{b=1}^{B} I[T_{b}^{\\ast RC} > T^{RC}]}\n",
    "$$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fdafb42-c24d-4741-aef2-c90e2ee47575",
   "metadata": {},
   "source": [
    "## __Intuition__\n",
    "\n",
    "* The boostrap means are like draws (simulation) from the asymptotic distribution $N(\\mathbf{0}, \\mathbf{\\Sigma})$\n",
    "\n",
    "* Taking the maximum of these draws simulates the distribution of a set of correlated normals\n",
    "\n",
    "* Each bootstrap mean is centered at the sample mean\n",
    "    - This is known as using the _Least Favorable Configuration_ (LFC) point\n",
    "    - Simulation is done assuming any model could be as good as the benchmark\n",
    "    \n",
    "* Since the asymptotic distribution can be simulated, asymptotic critical values and p-values can be constructed directly\n",
    "\n",
    "* The Monte Carlo Reality Check works by first estimating $\\Sigma$ using a HAC estimator, and then simulating random normals directly\n",
    "    - MCRC is equivalent to BRC, only requires estimating:\n",
    "        - A potentially large covariance if $m$ is big\n",
    "        - The Choleski decomposition of this covariance matrix\n",
    "        - B drawn from this Choleski\n",
    "    - In practice, $m$ may be so large that the covariance matrix won't fit in a normal computer's memory\n",
    "    \n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db84d98a-32de-4ed3-9777-c4d61cf6451a",
   "metadata": {},
   "source": [
    "## __Hansen's Test of SPA__\n",
    "\n",
    "* Hansen was White's doctoral student at UCSD\n",
    "\n",
    "* Hansen (2005, JBES) provided two refinements of the RC\n",
    "    1. Studentized loss differentials\n",
    "    2. Omission of very bad models from the distribution of the test statistic\n",
    "    \n",
    "* From a practical point-of-view, the first a very important consideration\n",
    "\n",
    "* From a theoretical point-of-view, the seocond is the important issue\n",
    "    - The second can be ignored if no models are very poor\n",
    "    - This may be difficult if using automated model generation schemes\n",
    "    \n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756f1d6a-946c-4aa6-bbfb-43d290a4d26e",
   "metadata": {},
   "source": [
    "## __Studentization of Loss Differentials__\n",
    "\n",
    "* The RC uses the loss differentials directly\n",
    "\n",
    "* This can lead to a loss of power if there is a large amount of cross-sectional heteroskedasticity\n",
    "\n",
    "* Bad, high variance model can mask a good, low variance model\n",
    "\n",
    "* The solution is to use the Studentized loss differential\n",
    "\n",
    "* The test statistic is based on\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "{\\large T^{SPA} = \\max_{j=1,\\ldots,m}{ \\left( \\frac{\\bar{\\delta}_{j}}{\\sqrt{\\hat{\\omega}_{j}^{2} / P}} \\right)}}\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "* $\\hat{\\omega}_{j}^{2}$ is an estimator of the asymptotic (long-run) variance of $\\bar{\\delta}_{j}$\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "{\\large \\hat{\\omega}_{j}^{2} = \\hat{\\gamma}_{j,0} + 2 \\sum_{i=1}^{P-1} k_{i} \\hat{\\gamma}_{j,i}}\n",
    "$$\n",
    "\n",
    "* $\\phantom{ }$\n",
    "    - $\\hat{\\gamma}_{j,i}$ is the $i^{th}$ sample autocovariance of the sequence $\\{\\delta_{j,t}\\}$\n",
    "    - $k_{i} = \\frac{P-i}{P} \\left(1 - \\frac{1}{w}\\right)^{i} + \\frac{i}{P} \\left(1 - \\frac{1}{w}\\right)^{P-i}$ where $w$ is the window lenght in the Stationary Bootstrap\n",
    "    \n",
    "<br>\n",
    "\n",
    "* Alternatively use bootstrap variance $\\hat{\\omega}_{j}^{2} = \\frac{P}{B} \\sum_{b=1}^{B} \\left(\\mathbf{\\bar{\\delta}}_{b,j}^{\\ast} - \\mathbf{\\bar{\\delta}}_{j}\\right)^{2}$\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58734cac-9a6c-40d2-b3bd-fd549f0105f7",
   "metadata": {},
   "source": [
    "### __The Algorithm (Studentized Bootstrap Reality Check)__\n",
    "\n",
    "---\n",
    "\n",
    "1. _Estimate_ $\\hat{\\omega}_{j}^{2}$ _and compute_ $T^{SPA} = \\max{\\left( \\bar{\\delta} / \\sqrt{\\hat{\\omega}_{j}^{2} / P} \\right)}$\n",
    "\n",
    "\n",
    "2. _For_ $b = 1, \\ldots, B$ _re-sample the vector of loss differentials_ $\\mathbf{\\delta}_{t}$ _to construct a bootstrap sample_ $\\{\\mathbf{\\delta}_{b,t}^{\\ast}\\}$ _using the stationary bootstrap_ \n",
    "\n",
    "\n",
    "3. _Using the bootstrap sample, compute\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "{\\large T_{u,b}^{\\ast SPA} = \\max{\\left( \\frac{P^{-1} \\sum_{t=R+1}^{T} (\\delta_{j,b,t}^{\\ast} - \\bar{\\delta}_{j})}{\\sqrt{\\hat{\\omega}_{j}^{2} / P}} \\right)}} \n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "4. _Compute the Studentized Reality Check p-value as the percentage of the boostrapped maxima which are larger than the sample maximum_\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "{\\large p-\\mbox{value} = \\frac{1}{B} \\sum_{b=1}^{B} I[T_{u,b}^{\\ast SPA} > T_{u}^{SPA}]}\n",
    "$$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1086b9-b01e-4cf3-b89d-3e8b0f45566e",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### __The__ $u$ __in__ $T_{u}^{SPA}$ __is for__ ___Upper___\n",
    "\n",
    "* The $U$ is included to indicate that the p-value derived using the LFC may not be the best p-value\n",
    "\n",
    "* Suppose that some of the models have a very low mean and a high standard deviation\n",
    "\n",
    "* In the RC and SPA-U, all models are assumed to be as good as the benchmark\n",
    "\n",
    "* This is implemented by always re-centering the bootstrap samples around $\\bar{\\delta}_{j}$\n",
    "\n",
    "* If a model is rejectably bad, then it may be possible to improve the power of the RC/SPA-U by excluding this model\n",
    "\n",
    "* This is implemented using a \"pre-test\" of the form\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "{\\large I_{j}^{u} = 1, \\quad I_{j}^{c} = \\frac{\\bar{\\delta}_{j}}{\\sqrt{\\hat{\\omega}}_{j}^{2}/P} > - \\sqrt{2 \\ln{\\ln{p}}}, \\quad I_{j}^{l} = \\bar{\\delta}_{j} > 0}\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "* The first ($c$ is for _consistent_) tests whether or not the standarized mean loss differential is greater than a HQ-like lower bound\n",
    "\n",
    "* The second ($l$ is for _lower_) only re-centers if the loss-differential is positive (e.g. the benchmark is out-performed)\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebfd0082-51d7-4b84-b5dc-0bec73f2e520",
   "metadata": {},
   "source": [
    "## __General SPA__\n",
    "\n",
    "### __Algorithm (Test of SPA)__\n",
    "\n",
    "---\n",
    "\n",
    "1. _Estimate_ $\\hat{\\omega}_{j}^{2}$ _and compute_ $T^{SPA} = \\max{\\left( \\bar{\\delta} / \\sqrt{\\hat{\\omega}_{j}^{2} / P} \\right)}$\n",
    "\n",
    "2. _For_ $b = 1, \\ldots, B$ _re-sample the vector of loss differentials_ $\\bar{\\delta}_{j}$ _to construct bootstrap sample_ $\\{ \\mathbf{\\delta}_{b,t}^{\\ast} \\}$ _using the stationary bootstrap_\n",
    "\n",
    "3. _Using the bootstrap sample, compute_\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "{\\large T_{s,b}^{\\ast SPA} = \\max{\\left( \\frac{P^{-1} \\sum_{t=R+1}^{T} (\\delta_{j,b,t}^{\\ast} - I_{j}^{s}\\bar{\\delta}_{j})}{\\sqrt{\\hat{\\omega}_{j}^{2} / P}} \\right)}, \\quad s = l, c, u} \n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "4. _Compute the Studentized Reality Check p-values as the percentage of the bootstrapped maxima which are larger than the sample maximum_\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "{\\large p-\\mbox{value} = \\frac{1}{B} \\sum_{b=1}^{B} I[T_{s,b}^{\\ast SPA} > T_{u}^{SPA}], \\quad s = l, c, u}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113438d6-5b91-4f7b-9961-c5b70cef6261",
   "metadata": {},
   "source": [
    "### __Comments on SPA__\n",
    "\n",
    "* The three versions only differ on whether or not a model is re-centered\n",
    "\n",
    "* If a model is _not_ re-centered, then it is unlikely to be the maximum in the re-sample distribution\n",
    "    - This is how \"bad\" models are discarded in the SPA\n",
    "    \n",
    "* Can compute 6 different p-values statistics\n",
    "    - Studentized or unmodified\n",
    "    - Indicator function in $l, c, u$\n",
    "        - Test statistic does not depend on $l, c, u$, only p-value does\n",
    "\n",
    "* Reality Check uses unmodified loss differentials and $u$\n",
    "\n",
    "* In practice Studentization bring important gains\n",
    "\n",
    "* Using $c$ is important if using SPA on large universe of automated rules if some may be very poor\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f865be8e-3c81-43c6-a9d4-14e35e1fa1da",
   "metadata": {},
   "source": [
    "## __Application of RC to Technical Trading Rules__\n",
    "\n",
    "* Sullivan, Timmermann, and White (1999) apply the RC to a large universe of technical trading rules\n",
    "\n",
    "* Rules include:\n",
    "    - Filter rules\n",
    "    - Moving Average Oscillators\n",
    "    - Support and Resistance\n",
    "    - Channel Breakout\n",
    "    - On-balance Volume Averages\n",
    "        - Tracks volume times return sign\n",
    "        - Similar to Moving Average rules for prices\n",
    "       \n",
    "* Total of 7,846 trading rules\n",
    "\n",
    "* Only use 1 at a time\n",
    "\n",
    "* Use DJIA as in BLL, updated to 1996\n",
    "\n",
    "* Consider mean return criteria and Sharpe Ratio\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b35980d-0427-4a76-adfc-2900c9814b62",
   "metadata": {},
   "source": [
    "### __Mean Return Performance BLL Universe__\n",
    "\n",
    "<br>\n",
    "\n",
    "![STW Table III](images/STW-Table-III.png)\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d7eb30-5c24-4136-a023-c684ba931adb",
   "metadata": {},
   "source": [
    "### __Sheppard's Notebook for SPA Demo__\n",
    "\n",
    "See here: https://github.com/bashtage/arch/blob/main/examples/multiple-comparison_examples.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60103957-c2ef-4dfd-9ee7-b8dea6b9ef5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from arch.bootstrap import SPA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91cce8c1-166b-4861-9b34-ea8e120a9265",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m\n",
       "\u001b[0mSPA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mbenchmark\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'ArrayLike'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmodels\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'ArrayLike'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mblock_size\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Optional[int]'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mreps\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'int'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mbootstrap\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"Literal[('stationary', 'sb', 'circular', 'cbb', 'moving block', 'mbb')]\"\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'stationary'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mstudentize\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'bool'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mnested\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'bool'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mseed\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Union[None, int, np.random.Generator, np.random.RandomState]'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m'None'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m     \n",
       "Test of Superior Predictive Ability (SPA) of White and Hansen.\n",
       "\n",
       "The SPA is also known as the Reality Check or Bootstrap Data Snooper.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "benchmark : {ndarray, Series}\n",
       "    T element array of benchmark model *losses*\n",
       "models : {ndarray, DataFrame}\n",
       "    T  by k element array of alternative model *losses*\n",
       "block_size : int, optional\n",
       "    Length of window to use in the bootstrap.  If not provided, sqrt(T)\n",
       "    is used.  In general, this should be provided and chosen to be\n",
       "    appropriate for the data.\n",
       "reps : int, optional\n",
       "    Number of bootstrap replications to uses.  Default is 1000.\n",
       "bootstrap : str, optional\n",
       "    Bootstrap to use.  Options are\n",
       "    'stationary' or 'sb': Stationary bootstrap (Default)\n",
       "    'circular' or 'cbb': Circular block bootstrap\n",
       "    'moving block' or 'mbb': Moving block bootstrap\n",
       "studentize : bool\n",
       "    Flag indicating to studentize loss differentials. Default is True\n",
       "nested=False\n",
       "    Flag indicating to use a nested bootstrap to compute variances for\n",
       "    studentization.  Default is False.  Note that this can be slow since\n",
       "    the procedure requires k extra bootstraps.\n",
       "seed : {int, Generator, RandomState}, optional\n",
       "    Seed value to use when creating the bootstrap used in the comparison.\n",
       "    If an integer or None, the NumPy default_rng is used with the seed\n",
       "    value.  If a Generator or a RandomState, the argument is used.\n",
       "\n",
       "Notes\n",
       "-----\n",
       "The three p-value correspond to different re-centering decisions.\n",
       "    - Upper : Never recenter to all models are relevant to distribution\n",
       "    - Consistent : Only recenter if closer than a log(log(t)) bound\n",
       "    - Lower : Never recenter a model if worse than benchmark\n",
       "\n",
       "See [1]_ and [2]_ for details.\n",
       "\n",
       "See Also\n",
       "--------\n",
       "StepM\n",
       "\n",
       "References\n",
       "----------\n",
       ".. [1] Hansen, P. R. (2005). A test for superior predictive ability.\n",
       "   Journal of Business & Economic Statistics, 23(4), 365-380.\n",
       ".. [2] White, H. (2000). A reality check for data snooping. Econometrica,\n",
       "   68(5), 1097-1126.\n",
       "\u001b[0;31mFile:\u001b[0m           ~/anaconda3/lib/python3.8/site-packages/arch/bootstrap/multiple_comparison.py\n",
       "\u001b[0;31mType:\u001b[0m           DocStringInheritor\n",
       "\u001b[0;31mSubclasses:\u001b[0m     RealityCheck\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "SPA?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77eea5af-4c5c-4f39-a161-95b5253f8690",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
