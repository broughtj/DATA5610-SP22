{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f61d6eaa-b502-46a7-b7d2-13548e4e46a0",
   "metadata": {},
   "source": [
    "# **Tree-Based Methods**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1605affe-f491-407a-97d8-d2f0bc305bd2",
   "metadata": {},
   "source": [
    "## Linear Decision Boundary with Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b31bcf-f039-4d0f-98f1-5671ac4d4d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_blobs, make_circles, make_classification, fetch_olivetti_faces\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.inspection import permutation_importance\n",
    "from xgboost import XGBClassifier\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99141a9-5b47-46d3-b3d7-f0fac0d83fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundary(X, y, clf):\n",
    "    x0_mesh, x1_mesh = np.meshgrid(np.arange(np.min(X[:,0])-0.5, np.max(X[:,0])+0.5, 0.1),\n",
    "                                 np.arange(np.min(X[:,1])-0.5, np.max(X[:,1])+0.5, 0.1))\n",
    "\n",
    "    y_mesh_pred = clf.predict(np.c_[x0_mesh.ravel(), x1_mesh.ravel()])\n",
    "    y_mesh_pred = y_mesh_pred.reshape(x0_mesh.shape)\n",
    "\n",
    "    plt.contourf(x0_mesh, x1_mesh, y_mesh_pred, cmap=\"RdBu\", vmin=0, vmax=1)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c = y, s = 50, edgecolor = 'w', cmap=\"RdBu\", vmin=-.2, vmax=1.2, linewidth=1)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6608fd4-9541-4d51-9082-da0debf41b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_classification(n_samples=1000, \n",
    "                           n_features=2, \n",
    "                           n_redundant=0, \n",
    "                           n_informative=2, \n",
    "                           random_state=10, \n",
    "                           n_clusters_per_class=1)\n",
    "\n",
    "x_train = X[:500,:]\n",
    "x_test = X[500:,:]\n",
    "y_train = y[:500]\n",
    "y_test = y[500:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e4ead2-eb02-4a1d-bb01-ef45f486ba3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x_train[:, 0], x_train[:, 1], c = y_train, s = 50, edgecolor = 'w', cmap=\"RdBu\", vmin=-.2, vmax=1.2, linewidth=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c605edc9-1ab2-47f2-91bd-a022c7e2a72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression().fit(x_train, y_train)\n",
    "train_acc = clf.score(x_train,y_train)\n",
    "test_acc = clf.score(x_test, y_test)\n",
    "\n",
    "print(f'Training Accuracy: {train_acc}')\n",
    "print(f'Test Accuracy: {test_acc}')\n",
    "plot_decision_boundary(x_test,y_test,clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb28c98-7509-4e6b-91b0-9c76e67b8d6b",
   "metadata": {},
   "source": [
    "## Non-Linear Decision Boundary with Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e04341a-2cf7-4f97-b83f-850be3c227ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_circles(n_samples=1000, noise=0.2, factor=0.01, random_state=1000)\n",
    "\n",
    "x_train = X[:500,:]\n",
    "x_val = X[500:600,:]\n",
    "x_test = X[600:,:]\n",
    "y_train = y[:500]\n",
    "y_val = y[500:600]\n",
    "y_test = y[600:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae63227-fe6d-4ae5-b5a3-62592f0e6aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x_train[:, 0], x_train[:, 1], c = y_train, s = 50, edgecolor = 'w', cmap=\"RdBu\", vmin=-.2, vmax=1.2, linewidth=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7363df4-8950-458f-8fb6-a041cfa69d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression().fit(x_train, y_train)\n",
    "train_acc = clf.score(x_train,y_train)\n",
    "test_acc = clf.score(x_test, y_test)\n",
    "\n",
    "print(f'Training Accuracy: {train_acc}')\n",
    "print(f'Test Accuracy: {test_acc}')\n",
    "plot_decision_boundary(x_test,y_test,clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d0731f-0e90-4709-866d-c193ccdb0690",
   "metadata": {},
   "source": [
    "## Decision Trees\n",
    "\n",
    "* Decision Trees can be applied to both classification and regression problems.\n",
    "* Can be thought of as partitioning the data space into into regions of similar points. Within those regions we aggregate results via voting (classification) or averaging (regression).\n",
    "\n",
    "### CART Algorithm\n",
    "To choose the first split:\n",
    "1. Sort all of the observed values from your training data for each predictor variable. Between every two values is a possible split point.\n",
    "2. Evaluate the performance of choosing each possible split point. \n",
    "    - For regression, this would imply averaging each training observation that would fall into either partition. This averaged value would be the predicted value of each observation in that partition and from this the MSE could be calculated.\n",
    "    - For classification, the process is identical except Gini impurity is used as the evaluation metric.\n",
    "3. The split point that results in the largest improvement of the evaluation metric is chosen.\n",
    "4. Repeat this process until some stopping criterion is reached.\n",
    "5. The terminal nodes (or leaves) of the tree will be the average or voted label of the training observations that fall into it. These determine the predicted value of a given observation.\n",
    "\n",
    "### Common Stopping Criterion\n",
    "- Max depth: The maximum depth of a tree where the \"depth\" is some number of consecutive partitions.\n",
    "- Minimum samples to split: The minimum number of samples necessary in a node for that node to be split on.\n",
    "- Minimum samples in a leaf node: The minimum number of samples that must be in each terminal (leaf) node for a split to be performed.\n",
    "- Minimum impurity decrease: A node will be split if it decreases the impurity beyond a predefined threshold. This helps avoid unnecessary splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2818e337-75a8-4161-887b-f209bc4761da",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = DecisionTreeClassifier().fit(x_train, y_train)\n",
    "train_acc = clf.score(x_train,y_train)\n",
    "test_acc = clf.score(x_test, y_test)\n",
    "\n",
    "print(f'Training Accuracy: {train_acc}')\n",
    "print(f'Test Accuracy: {test_acc}')\n",
    "plot_decision_boundary(x_test,y_test,clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86adb18-20d7-45e4-98d7-8876f6792f9e",
   "metadata": {},
   "source": [
    "### Pruning Trees\n",
    "\n",
    "If allowed to grow completely (i.e. no stopping criterion), a Decision Tree can achieve perfect performance (aside from any inherent measurement error). This often comes at the cost of the ability to generalize to unseen data, an important feature of predictive modeling. A smaller tree might lead to lower variance (less overfitting) at the cost of a slight increase in bias (increased underfitting). \n",
    "\n",
    "Choosing a stopping criterion would avoid this issue but it can be a bit too shortsighted. Using minimum impurity decrease, an early split might seem worthless but may lead to a very good split later on that might be missed if it doesn't exceed the impurity decrease threshold. We'll see an example of this with the XOR problem later on.\n",
    "\n",
    "Rather than choosing a stopping criterion a priori though, a more common approach is to \"prune\" a tree after it is grown to some large limit. Pruning involves recursively removing nodes to end up with a smaller subtree that should ideally generalize better to unseen data.\n",
    "\n",
    "#### Cost Complexity Pruning\n",
    "\n",
    "The most common method of pruning is referred to as cost complexity pruning (CCP), it also known as weakest link pruning. The complexity parameter, $\\alpha$ is used to define the cost-complexity measure, $R_{\\alpha}(T)$, of a given tree $T$, where\n",
    "\n",
    "$$\n",
    "R_{\\alpha}(T) = R(T) + \\alpha \\lvert T \\rvert\n",
    "$$\n",
    "\n",
    "where $\\lvert T\\rvert$ indicates the number of terminal nodes and $R(T)$ is the error rate of the tree. The cost complexity measure of a single node is $R_{\\alpha}(t) = R(t) + \\alpha$. The branch, $T_t$, is defined to be a tree where node $t$ is its root. In general, the impurity of a node is greater than the sum of impurities of its terminal nodes, $R(T_t) < R(t)$. However, the cost complexity measure of a node, $t$, and its branch, $T_t$, can be equal depending on $\\alpha$. We define the effective $\\alpha$ of a node to be the value where they are equal, $R_{\\alpha}(T_t)=R_{\\alpha}(t)$ or\n",
    "\n",
    "$$\n",
    "\\alpha_{eff}(t)=\\frac{R(t)-R(T_t)}{\\lvert T \\rvert -1}\n",
    "$$\n",
    "\n",
    "A non-terminal node with the smallest effective $\\alpha$ is the weakest link and will be pruned.\n",
    "\n",
    "Source: https://scikit-learn.org/stable/modules/tree.html#minimal-cost-complexity-pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3768a557-2cf2-4734-afdc-d2864feedaf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = clf.cost_complexity_pruning_path(x_train, y_train)\n",
    "ccp_alphas, impurities = path.ccp_alphas, path.impurities\n",
    "\n",
    "plt.plot(ccp_alphas, impurities)\n",
    "plt.xlabel(\"effective alpha\")\n",
    "plt.ylabel(\"total impurity of leaves\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c116a62-1bab-43e2-b427-c62355716cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "impurities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f8c270-421b-4b9b-8130-eb5dadbd05e3",
   "metadata": {},
   "source": [
    "This plot shows us the relationship between $\\alpha$ and the impurity of the leaves on the training data. Similarly, we can look at how those $\\alpha$'s relate to the depth of the trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76c06d1-ba7b-47f8-8405-fc410f60b597",
   "metadata": {},
   "outputs": [],
   "source": [
    "clfs = []\n",
    "\n",
    "for ccp_alpha in ccp_alphas:\n",
    "    clf = DecisionTreeClassifier(random_state=0, ccp_alpha=ccp_alpha)\n",
    "    clf.fit(x_train, y_train)\n",
    "    clfs.append(clf)\n",
    "    \n",
    "tree_depths = [clf.tree_.max_depth for clf in clfs]\n",
    "plt.figure(figsize=(10,  6))\n",
    "plt.plot(ccp_alphas[:-1], tree_depths[:-1])\n",
    "plt.xlabel(\"effective alpha\")\n",
    "plt.ylabel(\"total depth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a04d3f-de5d-4a14-8e80-50f3bef9b86e",
   "metadata": {},
   "source": [
    "These effective $\\alpha$ values were determined based on the training set, meaning that they will be susceptible to overfitting stiil. To counter this, we compare different subtrees for the various values of $\\alpha$ with the resulting accuracy on a holdout validation set.\n",
    "\n",
    "Note: This is often done via cross-validation but is simply shown here with a single validation set for brevity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05acbc2-1dcf-4758-b8e0-1991cd887d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_scores = [clf.score(x_val, y_val) for clf in clfs]\n",
    "\n",
    "tree_depths = [clf.tree_.max_depth for clf in clfs]\n",
    "plt.grid()\n",
    "plt.plot(ccp_alphas[:-1], acc_scores[:-1])\n",
    "plt.xlabel(\"alpha\")\n",
    "plt.ylabel(\"Accuracy scores\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61458f00-9a51-4ceb-b7cf-b8203e0e6528",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_clf = clfs[np.argmax(acc_scores)]\n",
    "\n",
    "train_acc = optimal_clf.score(x_train,y_train)\n",
    "test_acc = optimal_clf.score(x_test, y_test)\n",
    "\n",
    "print(f'Training Accuracy: {train_acc}')\n",
    "print(f'Test Accuracy: {test_acc}')\n",
    "\n",
    "plot_decision_boundary(x_test,y_test,optimal_clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c63752e-5d5b-40bb-980d-9229c955c4ce",
   "metadata": {},
   "source": [
    "### Trees are not always optimal (XOR)\n",
    "\n",
    "Decision Trees are what is referred to as a greedy algorithm, since they will take a nearsighted view of splitting, selecting the best available split without considering the future. This can lead to non-optimal results as evidenced in the simple example of the XOR problem, where there the optimal splits would be at 0 on either dimension but either of those splits from the root node would lead to an error rate of about 50%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7951350-41dd-48e4-82c9-c55c6b6a69e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xor(n=1000):\n",
    "    X = (-1 - 1) * np.random.random_sample((n,2)) + 1\n",
    "    \n",
    "    y = []\n",
    "    for i in range(X.shape[0]):\n",
    "        if X[i,0] <= 0 and X[i,1] > 0 or X[i,0] > 0 and X[i,1] <= 0:\n",
    "            y.append(1)\n",
    "        else:\n",
    "            y.append(0)\n",
    "    return X, y\n",
    "\n",
    "X, y = xor()\n",
    "plt.scatter(X[:, 0], X[:, 1], c = y, s = 50, edgecolor = 'w', cmap=\"RdBu\", vmin=-.2, vmax=1.2, linewidth=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31cf54c1-4538-448b-af85-10e67b9c9537",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = DecisionTreeClassifier().fit(X, y)\n",
    "train_acc = clf.score(X,y)\n",
    "\n",
    "print(f'Training Accuracy: {train_acc}')\n",
    "plot_decision_boundary(X,y,clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c169bb-2679-4ff4-a01a-0dfde14ed0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "plot_tree(clf)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b43157-dcde-4aac-bac7-28e17209b1ae",
   "metadata": {},
   "source": [
    "### Pros and Cons of Decision Trees\n",
    "Pros:\n",
    "- Easy to interpret\n",
    "- Able to handle both numerical and categorical\n",
    "- Doesn't require much data prep (e.g. no need to normalize values)\n",
    "- Can be handed off to non-data people and still be understood\n",
    "\n",
    "Cons:\n",
    "- Low bias but high variance.\n",
    "    - Very prone to overfitting\n",
    "    - A slight change in the inputs can result in significantly different tree structure\n",
    "- For categorical variables, information gain in Decision Trees is highly biased towards variables with a higher number of levels.\n",
    "- Time complexity isn't great. During training trees are $O(nlogn * d)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c471b926-fabd-4e9f-ac44-08d6b8684d10",
   "metadata": {},
   "source": [
    "## Bias-Variance Tradeoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6ef1df-f5c3-4a80-8201-f3704e13a76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_classification(\n",
    "    n_features=2, n_redundant=0, n_informative=2, random_state=100, n_clusters_per_class=1\n",
    ")\n",
    "\n",
    "x_train = X[:50,:]\n",
    "x_test = X[50:,:]\n",
    "y_train = y[:50]\n",
    "y_test = y[50:]\n",
    "\n",
    "clf = DecisionTreeClassifier().fit(X, y)\n",
    "train_acc = clf.score(x_train,y_train)\n",
    "test_acc = clf.score(x_test, y_test)\n",
    "\n",
    "print(f'Training Accuracy: {train_acc}')\n",
    "print(f'Test Accuracy: {test_acc}')\n",
    "plot_decision_boundary(x_test,y_test,clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e42a21-6e49-48be-8d9a-3de0b90f930e",
   "metadata": {},
   "source": [
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/9/9f/Bias_and_variance_contributing_to_total_error.svg/2560px-Bias_and_variance_contributing_to_total_error.svg.png\" width=400 height=400 />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851abceb-7753-4208-913b-621367b86dd6",
   "metadata": {},
   "source": [
    "<img src=\"https://www.cs.cornell.edu/courses/cs4780/2018fa/lectures/images/bias_variance/bullseye.png\" height=400 width=400 />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bddfbe6-8b72-4fec-a186-d7db97677749",
   "metadata": {},
   "source": [
    "# Random Forest\n",
    "\n",
    "## Bootstrap Aggregating (Bagging)\n",
    "As stated before, Decision Trees are very prone to overfitting, such that a small change in the inputs can have a dramatic effect on the tree that is grown. One way to counter this would be to train the Decision Tree on a bootstrapped sample of the data. A bootstrap sample is a dataset that is sampled with replacement to be as large as the original data. We can create infinitely many bootstrap samples and thus train infinitely many different trees on those samples. Each of these trees, however, would still exhibit high variance (overfit) to their respective bootstrap sample.\n",
    "\n",
    "By aggregating the predictions from each tree together though, we should expect to see a decrease in the variance. This is the origin of the term Bagging a portmanteau of Bootstrap and Aggregating. Bagging can be utilized with almost any Machine Learning method, even Linear Regression, but it has been shown to be especially effective with Decision Trees.\n",
    "\n",
    "## Random Forests\n",
    "Random Forests (Breiman, 2001) take this idea of Bagging one step further though, with a goal of further reducing the variance with little to no increase in bias. The issue with simply fitting trees using bagging is that each tree is still being trained on the same set of predictors, so, assuming a true random sample, these trees won't necessarily be significantly different than each other. Errors in one tree will likely be present in many others, so Random Forests seek to decorrelate the trees even further.\n",
    "\n",
    "The algorithm is as follows:\n",
    "1. Fit a large number of Decision Trees on bootstrapped samples of the training data.\n",
    "2. For each split, rather than considering every possible variable to split on, randomly select a subset of predictor variables and only sort and evaluate the splits along those variables.\n",
    "3. Grow each tree to max depth (to reduce bias)\n",
    "4. Vote (classification) or average (regression) each predicted value across all trees to perform the final prediciton.\n",
    "\n",
    "These decorrellated trees typically show significantly reduced variance without increasing the low bias that is common in Decision Trees.\n",
    "\n",
    "Random Forests are among a class of models referred to as ensembles since they are a collection of underlying models or learners. In the case of Random Forests, all of the underlying models are Decision Trees so this would be a homogeneous ensemble, while an ensemble of differing underlying models would unsurprisingly be referred to as heterogeneous. Regardless of whether the ensemble is homo- or heterogeneous though, the ensemble performs best when the underlying models are not correlated (i.e. make mistakes in different places).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422c5bc8-66a8-451f-a6f5-77a236d622c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators=100,\n",
    "                            max_depth=None,\n",
    "                            oob_score=True)\n",
    "clf.fit(x_train, y_train)\n",
    "train_acc = clf.score(x_train,y_train)\n",
    "test_acc = clf.score(x_test, y_test)\n",
    "\n",
    "print(f'Training Accuracy: {train_acc}')\n",
    "print(f'Out-of-bag Accuracy: {clf.oob_score_}')\n",
    "print(f'Test Accuracy: {test_acc}')\n",
    "plot_decision_boundary(x_test,y_test,clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff663005-9d86-4352-b945-0a46b9a3f55f",
   "metadata": {},
   "source": [
    "### n << p\n",
    "\n",
    "A by product of the algorithmic choice to subsample predictors for each split, is that Random Forests can be trained on datasets with a large number of variables, even if the number of variables far exceeds the number of predictors.\n",
    "\n",
    "Note: This is only a computational benefit and does not necessarily help with the overfitting assocaited with $n<<p$ scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0fe5ba0-4936-4962-9cd7-dde17ed2454e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: https://archive.ics.uci.edu/ml/datasets/gene+expression+cancer+RNA-Seq\n",
    "df = pd.read_csv(\"/Users/sharad/Courses/DATA 5610/Data/TCGA-PANCAN-HiSeq-801x20531/data.csv\")\n",
    "df.drop(columns=['Unnamed: 0'],inplace=True)\n",
    "labels = pd.read_csv(\"/Users/sharad/Courses/DATA 5610/Data/TCGA-PANCAN-HiSeq-801x20531/labels.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772cc0e8-b7e3-487d-8051-647b39c4b2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.to_numpy()\n",
    "y = labels.to_numpy()\n",
    "y = y[:,1]\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "del(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d93bd3-8748-469c-915f-cc4c7b5de0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc967cc-1382-4592-903b-7bd6339d5c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators=100,\n",
    "                            max_depth=None,\n",
    "                            max_features=25,\n",
    "                            oob_score=True)\n",
    "clf.fit(X_train,y_train)\n",
    "\n",
    "train_acc = clf.score(X_train,y_train)\n",
    "test_acc = clf.score(X_test, y_test)\n",
    "\n",
    "print(f'Training Accuracy: {train_acc}')\n",
    "print(f'Out-of-bag Accuracy: {clf.oob_score_}')\n",
    "print(f'Test Accuracy: {test_acc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5762913-8b0b-465f-9b08-a5da04925077",
   "metadata": {},
   "source": [
    "### Sensitivity to hyperparmeters\n",
    "\n",
    "Another benefit of Random Forests to the Machine Learning practitioner is that they are robust to the selection of their hyperparameters, making them an easy first choice for model selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a9b811-5e35-490a-9706-5aa41a8e34b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = fetch_olivetti_faces()\n",
    "X, y = data.data, data.target\n",
    "mask = y < 30\n",
    "X = X[mask]\n",
    "y = y[mask]\n",
    "\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3172d0-0f63-45b5-85e0-d126ede133d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_trees = [1,10,25,100,500,1000,5000]\n",
    "\n",
    "oob_acc = []\n",
    "for n in n_trees:\n",
    "    clf = RandomForestClassifier(n_estimators=n,\n",
    "                                 max_depth=None,\n",
    "                                 max_features=25,\n",
    "                                 oob_score=True)\n",
    "    clf.fit(X,y)\n",
    "    oob_acc.append(clf.oob_score_)\n",
    "\n",
    "plt.grid()\n",
    "plt.ylim([0, 1.0])\n",
    "plt.plot(n_trees, oob_acc)\n",
    "plt.xlabel(\"Number of Trees\")\n",
    "plt.ylabel(\"OOB Accuracy\")\n",
    "print(oob_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb150d4-50c3-46a3-8102-50253ddbcc42",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features_per_split = [1,10,25,50,100,500]\n",
    "\n",
    "oob_acc = []\n",
    "for n in n_features_per_split:\n",
    "    clf = RandomForestClassifier(n_estimators=100,\n",
    "                                 max_depth=None,\n",
    "                                 max_features=n,\n",
    "                                 oob_score=True)\n",
    "    clf.fit(X,y)\n",
    "    oob_acc.append(clf.oob_score_)\n",
    "\n",
    "plt.grid()\n",
    "plt.ylim([0, 1.0])\n",
    "plt.plot(n_features_per_split, oob_acc)\n",
    "plt.xlabel(\"Number of Features\")\n",
    "plt.ylabel(\"OOB Accuracy\")\n",
    "print(oob_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea593d96-c419-473d-a45a-0b97b04b5764",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depth = [1,5,10,100,1000,5000]\n",
    "\n",
    "oob_acc = []\n",
    "for n in max_depth:\n",
    "    clf = RandomForestClassifier(n_estimators=100,\n",
    "                                 max_depth=n,\n",
    "                                 max_features=25,\n",
    "                                 oob_score=True)\n",
    "    clf.fit(X,y)\n",
    "    oob_acc.append(clf.oob_score_)\n",
    "\n",
    "plt.grid()\n",
    "plt.ylim([0, 1.0])\n",
    "plt.plot(max_depth, oob_acc)\n",
    "plt.xlabel(\"Max Depth\")\n",
    "plt.ylabel(\"OOB Accuracy\")\n",
    "print(oob_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ab03ca-2486-433f-936f-5678562a7c6c",
   "metadata": {},
   "source": [
    "# AdaBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03235476-7c6f-43b7-81ad-3c9edaedce1b",
   "metadata": {},
   "source": [
    "An alternate approach to ensembling trees that grows the ensemble sequentially is called Boosting. Boosting has become the go-to method for improving classification accuracy on structured datasets, as evidenced by many of the top Kaggle models. This increase in accuracy does come at the cost of explainability but if accuracy is what matters, these are a strong candidate.\n",
    "\n",
    "Arguably the simplest boosting algorithm to understand is AdaBoost (short for Adaptive Boosting). The underlying motivations of this algorithm carry over in to many of the more advanced methods, so a good understanding of AdaBoost is critical.\n",
    "\n",
    "Rather than fitting trees to max depth, as with Random Forests, AdaBoost takes a different extreme by fitting what is referred to as a Decision Stump, a tree consisting of a single split. \n",
    "To grow this stump, \n",
    "1. Identical weights are given to all of the observations and a weighted sample is performed to select the data for that split. \n",
    "2. The split can then be found identically to how CART would, by sorting and searching through all predictors. The Decision Stump is often referred to as a weak learner, since it will likely have a very high error rate due to severely underfitting the data. \n",
    "3. For the next tree though, the training data will be passed through the stump and misclassified observations will be upweighted, while correctly classified examples are downweighted.\n",
    "4. Resample with the adjusted (adapted) weights and grow another stump.\n",
    "5. Repeat until some stopping criterion is reached (e.g. no errors on training data).\n",
    "\n",
    "### Hyperparameters\n",
    "- Max number of stumps: The maximum number of stumps to train before ending training.\n",
    "- Learning rate: Weight applied to each classifier at each boosting iteration. A higher learning rate increases the contribution of each classifier.\n",
    "\n",
    "### Disadvantages\n",
    "- Must be grown sequentially (i.e. not easily parallelizable)\n",
    "- Sensitive to outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4142515c-842e-4f8b-8f0e-35c2016c0872",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Users/sharad/Courses/DATA 5610/Data/pima-indians-diabetes.csv', names=['Pregnancies',\n",
    "                                                                                           'Glucose',\n",
    "                                                                                           'BP',\n",
    "                                                                                           'SkinThickness',\n",
    "                                                                                           'Insulin',\n",
    "                                                                                           'BMI',\n",
    "                                                                                           'DiabetesPedigreeFunction',\n",
    "                                                                                           'Age',\n",
    "                                                                                           'Class'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d49485f-6143-41bd-81fb-8832ef32816f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:,0:8]\n",
    "y = df.iloc[:,8]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=24)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992f5c1f-743b-499b-8dc2-482f2ecd371f",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators=100,\n",
    "                            max_depth=None,\n",
    "                            oob_score=True)\n",
    "clf.fit(X_train,y_train)\n",
    "\n",
    "train_acc = clf.score(X_train,y_train)\n",
    "test_acc = clf.score(X_test, y_test)\n",
    "\n",
    "print(f'Training Accuracy: {train_acc}')\n",
    "print(f'Out-of-bag Accuracy: {clf.oob_score_}')\n",
    "print(f'Test Accuracy: {test_acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4dc5317-e774-4f90-808f-16e07590a9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = AdaBoostClassifier(n_estimators=1000, learning_rate=0.01)\n",
    "\n",
    "clf.fit(X_train,y_train)\n",
    "train_acc = clf.score(X_train,y_train)\n",
    "cv_scores = cross_val_score(clf, X_train, y_train, cv=5)\n",
    "test_acc = clf.score(X_test, y_test)\n",
    "\n",
    "print(f'Training Accuracy: {train_acc}')\n",
    "print(f'Cross-validated Accuracy: {np.mean(cv_scores)}')\n",
    "print(f'Test Accuracy: {test_acc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52d0c49-1cb5-4530-9c58-1280b19da280",
   "metadata": {},
   "source": [
    "# XGBoost\n",
    "\n",
    "Extreme Gradient Boosting (XGBoost) has become one of the most popular algorithms for predictive modeling on structured datasets. Gradient Boosting Machines (GBM) underly XGBoost and can be viewed themselves as a generalization of AdaBoost, where the boosting procedure is structured as an optimization problem with an objective of minimizing the loss by adding weak learners, resembling something like gradient descent.\n",
    "\n",
    "GBMs require the user to define a differentiable loss function such as MSE or cross-entropy based on their target or outcome variable. Then weak learners are added to the model such that they decrease this loss (i.e. follow the gradient) until some stopping criterion is reached.\n",
    "\n",
    "XGBoost extends these properties a bit more by adding shrinkage and feature subsampling. Shrinkage scales newly added weights by a factor $\\eta$ after each iteration. Similar to a learning rate in stochastic optimization, shrinkage reduces the influence of each tree and leaves space for future trees to improve the model. Feature subsampling is identical to the subsampling of predictors as performed in Random Forests and serves the same purpose of decorellating the trees.\n",
    "\n",
    "Most of the other improvements associated with XGBoost are computational as a main focus of the algorithm was improved computational efficiency. These improvements include: efficient splitting algorithms, parallelization of tree construction, cache-aware access, distributed computing.\n",
    "\n",
    "Source: https://www.mygreatlearning.com/blog/xgboost-algorithm/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15eb533-f14f-4c8a-b097-83cfab99f15b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "clf = XGBClassifier(use_label_encoder=False, \n",
    "                    eval_metric='error', \n",
    "                    n_estimators= 2000,\n",
    "                    max_depth= 9,\n",
    "                    min_child_weight= 2,\n",
    "                    gamma=0.4,\n",
    "                    subsample=0.8,\n",
    "                    colsample_bytree=0.8,\n",
    "                    objective= 'binary:logistic',\n",
    "                    nthread= -1,\n",
    "                    scale_pos_weight=1)\n",
    "clf.fit(X_train, y_train)\n",
    "train_acc = clf.score(X_train,y_train)\n",
    "test_acc = clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc572e7-78ce-45b2-97c1-3e6432bf64e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Training Accuracy: {train_acc}')\n",
    "print(f'Test Accuracy: {test_acc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579c7130-e68b-44ab-a746-bd9280fbd394",
   "metadata": {},
   "source": [
    "## Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075ba837-8f8f-412f-8800-7b50280ab0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_circles(n_samples=100, noise=0.2, factor=0.01, random_state=1000)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715c3069-4c07-4a7c-aa4b-53c1ba1f6153",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "folds = np.random.randint(0,10,X_train.shape[0])\n",
    "folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4d2c6d-5c5d-4b2f-86f1-2439b2f91c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_fold_preds = []\n",
    "val_fold_true = []\n",
    "\n",
    "for i in range(10):\n",
    "    train_fold_x = X_train[folds != i,:]\n",
    "    train_fold_y = y_train[folds != i]\n",
    "    val_fold_x = X_train[folds == i,:]\n",
    "    val_fold_y = y_train[folds == i]\n",
    "    \n",
    "    clf = DecisionTreeClassifier().fit(train_fold_x, train_fold_y)\n",
    "    val_fold_preds.extend(clf.predict(val_fold_x))\n",
    "    val_fold_true.extend(val_fold_y)\n",
    "\n",
    "confusion_matrix(val_fold_true, val_fold_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a5f7ad-91fc-4ca8-bf4e-991063c435e9",
   "metadata": {},
   "source": [
    "## Feature Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cda82f7-049e-49ab-ae44-2117e2212098",
   "metadata": {},
   "source": [
    "Given the out-of-the-box usability and natural handling of collinearity, Random Forests are a strong candidate for performing feature selection when presented with a large number of variables or simply for a post hoc feature importance assessment. There are two common ways to assess feature importance for Random Forests: impurity based and permutation based.\n",
    "\n",
    "### Impurity Based\n",
    "\n",
    "When training a Decision Tree, the splits are chosen to decrease the impurity in child nodes. Feature importance is calculated as the decrease in node impurity weighted by the probability of reaching that node. The node probability can be calculated by the number of samples that reach the node, divided by the total number of samples. The higher the value the more important the feature. For Random Forests, this is average across all trees resulting in an estimate of importance for each feature in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e229f7-307e-4e2b-9e18-adfdb761c8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "colnames = ['Pregnancies','Glucose','BP','SkinThickness','Insulin','BMI','DiabetesPedigreeFunction','Age','Class']\n",
    "df = pd.read_csv('/Users/sharad/Courses/DATA 5610/Data/pima-indians-diabetes.csv', names=colnames)\n",
    "\n",
    "X = df.iloc[:,0:8]\n",
    "y = df.iloc[:,8]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=24)\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=100,\n",
    "                            max_depth=None,\n",
    "                            oob_score=True)\n",
    "clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8e7450-70b9-4cde-a904-2ea9daa3c9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(x=colnames[:8],height=clf.feature_importances_)\n",
    "plt.xticks(rotation = 90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01cb522f-463b-49fe-a678-14afc283f467",
   "metadata": {},
   "source": [
    "### Permutation Based\n",
    "\n",
    "The impurity based approach has a few drawbacks that are worth considering though. First, it can bias towards high cardinality or continuous features due to them having a large number of possible splits and sometimes being over represented in a tree. This approach also only estimates importance based on training data so an overfit model may provide incorrect feature importance.\n",
    "\n",
    "The permutation based approach accounts for these shortcomings and provides a few more benefits as well. One major strength is that this approach is more general in that it can be applied to any supervised learning model. To estimate the importance of a feature, one-by-one, the values of each feature are randomly shuffled and passed back through the trained model. The decrease in the model performance when that feature is shuffled is deemed the \"importance\". This shuffling is typically repeated multiple times and averaged for each variable. \n",
    "\n",
    "A strength of this approach is that it can be estimated on holdout or test data, which allows overfitting to the training set to be accounted for. It is common to estimate the permutation based feature importance via cross-validation if compute resources allow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60fe7380-7d5b-4503-b091-82abfe160fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_perm_imp = []\n",
    "folds = np.random.randint(0,10,X_train.shape[0])\n",
    "\n",
    "for i in range(10):\n",
    "    train_fold_x = X_train[folds != i]\n",
    "    train_fold_y = y_train[folds != i]\n",
    "    val_fold_x = X_train[folds == i]\n",
    "    val_fold_y = y_train[folds == i]\n",
    "    \n",
    "    clf = RandomForestClassifier().fit(train_fold_x, train_fold_y)\n",
    "    \n",
    "    perm_imp = permutation_importance(clf, val_fold_x, val_fold_y, n_repeats = 30, random_state=24)\n",
    "    cv_perm_imp.append(perm_imp['importances_mean'])\n",
    "\n",
    "cv_perm_imp = np.stack(cv_perm_imp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7a1a97-5bad-4387-a1c8-712ad1e51089",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "plt.boxplot(cv_perm_imp, labels = colnames[:8], showmeans=True)\n",
    "plt.xticks(rotation = 90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6da4fc3-a3a8-477d-a3bd-ebb52c65792b",
   "metadata": {},
   "source": [
    "## Partial Dependence Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1ee017-8779-41a3-98a3-c5d13050aa02",
   "metadata": {},
   "source": [
    "Partial Dependence Plots (PDP) give us another way of interpreting the model's behavior. The goal of PDPs though is not importance though but rather the change of the target variable for changing values of one or more input variables. The main output of this approach is a visualization of this relationship, therefore, these plots are constrained to one- or two-way plots, which means we can either look at the effect of one variable on the target variable or on the interaction between two variables. Due to this constraint, it can be valuable to assess the most important features first and then visualize these via PDPs.\n",
    "\n",
    "At a high level, PDPs are created by holding all features in the model fixed except for the feature of interest. The feature of interest is then varied along the range of observed values and the predicted output is stored. This can be repeated for varying values of the fixed input features and all results averaged to give a final plot of the typical change in the target for changes in the input feature. A similar process is taken for the two-way PDP.\n",
    "\n",
    "The biggest issue of PDPs are that they assume features are truly independent. This is a very difficult assumption to confirm in real data so all PDPs should be interpreted with this caveat in mind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a698e5ce-1c70-44d3-8f52-1f28ffd8f819",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import PartialDependenceDisplay\n",
    "\n",
    "features = [1, 5]\n",
    "PartialDependenceDisplay.from_estimator(clf, X_train, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef08078b-5e17-4576-9bb6-0c71b27e6f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [(1, 5)]\n",
    "PartialDependenceDisplay.from_estimator(clf, X_train, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7078db7-0dae-44a7-a269-6f7d06ff6fb1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
